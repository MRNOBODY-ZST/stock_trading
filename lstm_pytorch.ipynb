{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate,\n",
    "        num_layers,\n",
    "        size,\n",
    "        size_layer,\n",
    "        output_size,\n",
    "        forget_bias=0.1,\n",
    "    ):\n",
    "        super(LstmModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=size,\n",
    "            hidden_size=size_layer,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=1 - forget_bias\n",
    "        )\n",
    "        self.fc = nn.Linear(size_layer, output_size)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        output, (hn, cn) = self.lstm(x, hidden)\n",
    "        return self.fc(output[:, -1, :]), (hn, cn)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(self.lstm.num_layers, batch_size, self.lstm.hidden_size).zero_(),\n",
    "                weight.new(self.lstm.num_layers, batch_size, self.lstm.hidden_size).zero_())\n",
    "\n",
    "def calculate_accuracy(real, predict):\n",
    "    real = np.array(real) + 1\n",
    "    predict = np.array(predict) + 1\n",
    "    percentage = 1 - np.sqrt(np.mean(np.square((real - predict) / real)))\n",
    "    return percentage * 100\n",
    "\n",
    "def anchor(signal, weight):\n",
    "    buffer = []\n",
    "    last = signal[0]\n",
    "    for i in signal:\n",
    "        smoothed_val = last * weight + (1 - weight) * i\n",
    "        buffer.append(smoothed_val)\n",
    "        last = smoothed_val\n",
    "    return buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-11-02</td>\n",
       "      <td>778.200012</td>\n",
       "      <td>781.650024</td>\n",
       "      <td>763.450012</td>\n",
       "      <td>768.700012</td>\n",
       "      <td>768.700012</td>\n",
       "      <td>1872400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-11-03</td>\n",
       "      <td>767.250000</td>\n",
       "      <td>769.950012</td>\n",
       "      <td>759.030029</td>\n",
       "      <td>762.130005</td>\n",
       "      <td>762.130005</td>\n",
       "      <td>1943200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-11-04</td>\n",
       "      <td>750.659973</td>\n",
       "      <td>770.359985</td>\n",
       "      <td>750.560974</td>\n",
       "      <td>762.020020</td>\n",
       "      <td>762.020020</td>\n",
       "      <td>2134800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-11-07</td>\n",
       "      <td>774.500000</td>\n",
       "      <td>785.190002</td>\n",
       "      <td>772.549988</td>\n",
       "      <td>782.520020</td>\n",
       "      <td>782.520020</td>\n",
       "      <td>1585100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-11-08</td>\n",
       "      <td>783.400024</td>\n",
       "      <td>795.632996</td>\n",
       "      <td>780.190002</td>\n",
       "      <td>790.510010</td>\n",
       "      <td>790.510010</td>\n",
       "      <td>1350800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date        Open        High         Low       Close   Adj Close  \\\n",
       "0  2016-11-02  778.200012  781.650024  763.450012  768.700012  768.700012   \n",
       "1  2016-11-03  767.250000  769.950012  759.030029  762.130005  762.130005   \n",
       "2  2016-11-04  750.659973  770.359985  750.560974  762.020020  762.020020   \n",
       "3  2016-11-07  774.500000  785.190002  772.549988  782.520020  782.520020   \n",
       "4  2016-11-08  783.400024  795.632996  780.190002  790.510010  790.510010   \n",
       "\n",
       "    Volume  \n",
       "0  1872400  \n",
       "1  1943200  \n",
       "2  2134800  \n",
       "3  1585100  \n",
       "4  1350800  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../dataset/GOOG-year.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.112708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.090008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.089628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.160459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.188066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0  0.112708\n",
       "1  0.090008\n",
       "2  0.089628\n",
       "3  0.160459\n",
       "4  0.188066"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minmax = MinMaxScaler().fit(df.iloc[:, 4:5].astype('float32')) # Close index\n",
    "df_log = minmax.transform(df.iloc[:, 4:5].astype('float32')) # Close index\n",
    "df_log = pd.DataFrame(df_log)\n",
    "df_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((252, 7), (222, 1), (30, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = 30\n",
    "simulation_size = 10\n",
    "\n",
    "df_train = df_log.iloc[:-test_size]\n",
    "df_test = df_log.iloc[-test_size:]\n",
    "df.shape, df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 1\n",
    "size_layer = 128\n",
    "timestamp = 5\n",
    "epoch = 50\n",
    "dropout_rate = 0.8\n",
    "future_day = test_size\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(df_log.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast(model, df, df_log, df_train, minmax, learning_rate, num_layers, size_layer, dropout_rate, epoch, timestamp, test_size):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    date_ori = pd.to_datetime(df.iloc[:, 0]).tolist()\n",
    "\n",
    "    pbar = tqdm(range(epoch), desc='train loop')\n",
    "    for _ in pbar:\n",
    "        total_loss, total_acc = [], []\n",
    "        h_state = model.init_hidden(1)\n",
    "        \n",
    "        for k in range(0, df_train.shape[0] - 1, timestamp):\n",
    "            # 重置梯度\n",
    "            model.zero_grad()\n",
    "            \n",
    "            index = min(k + timestamp, df_train.shape[0] - 1)\n",
    "            batch_x = torch.FloatTensor(df_train.iloc[k:index, :].values).unsqueeze(0).to(device)\n",
    "            batch_y = torch.FloatTensor(df_train.iloc[k+1:index+1, :].values).to(device)\n",
    "            \n",
    "            # 分离隐藏状态，避免梯度累积\n",
    "            h_state = (h_state[0].detach(), h_state[1].detach())\n",
    "            \n",
    "            logits, h_state = model(batch_x, h_state)\n",
    "            loss = criterion(logits, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss.append(loss.item())\n",
    "            total_acc.append(calculate_accuracy(batch_y[:, 0].cpu().numpy(), logits[:, 0].detach().cpu().numpy()))\n",
    "        \n",
    "        pbar.set_postfix(cost=np.mean(total_loss), acc=np.mean(total_acc))\n",
    "\n",
    "    future_day = test_size\n",
    "    output_predict = np.zeros((df_train.shape[0] + future_day, df_train.shape[1]))\n",
    "    output_predict[0] = df_train.iloc[0]\n",
    "    upper_b = (df_train.shape[0] // timestamp) * timestamp\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        h_state = model.init_hidden(1)\n",
    "        for k in range(0, upper_b, timestamp):\n",
    "            x = torch.FloatTensor(df_train.iloc[k:k+timestamp].values).unsqueeze(0).to(device)\n",
    "            out, h_state = model(x, h_state)\n",
    "            output_predict[k+1:k+timestamp+1] = out.cpu().numpy()\n",
    "\n",
    "        if upper_b != df_train.shape[0]:\n",
    "            x = torch.FloatTensor(df_train.iloc[upper_b:].values).unsqueeze(0).to(device)\n",
    "            out, h_state = model(x, h_state)\n",
    "            output_predict[upper_b+1:df_train.shape[0]+1] = out.cpu().numpy()\n",
    "            future_day -= 1\n",
    "            date_ori.append(date_ori[-1] + timedelta(days=1))\n",
    "\n",
    "        for i in range(future_day):\n",
    "            x = torch.FloatTensor(output_predict[-future_day-timestamp+i:-future_day+i]).unsqueeze(0).to(device)\n",
    "            out, h_state = model(x, h_state)\n",
    "            output_predict[-future_day+i] = out[-1].cpu().numpy()\n",
    "            date_ori.append(date_ori[-1] + timedelta(days=1))\n",
    "\n",
    "    output_predict = minmax.inverse_transform(output_predict)\n",
    "    deep_future = anchor(output_predict[:, 0], 0.3)\n",
    "    \n",
    "    return deep_future[-test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulations(model_class, df, df_log, df_train, minmax, learning_rate, num_layers, size_layer, dropout_rate, epoch, timestamp, test_size, simulation_size):\n",
    "    results = []\n",
    "    for i in range(simulation_size):\n",
    "        print(f'simulation {i + 1}')\n",
    "        # 为每次模拟创建一个新的模型实例\n",
    "        model = model_class(\n",
    "            learning_rate=learning_rate,\n",
    "            num_layers=num_layers,\n",
    "            size=df_log.shape[1],\n",
    "            size_layer=size_layer,\n",
    "            output_size=df_log.shape[1],\n",
    "            forget_bias=dropout_rate\n",
    "        )\n",
    "        result = forecast(model, df, df_log, df_train, minmax, learning_rate, num_layers, size_layer, dropout_rate, epoch, timestamp, test_size)\n",
    "        results.append(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hades\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.19999999999999996 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulation 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loop:   0%|          | 0/50 [00:00<?, ?it/s]c:\\Users\\hades\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([5, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "train loop: 100%|██████████| 50/50 [00:03<00:00, 15.90it/s, acc=97.4, cost=0.00158] \n",
      "c:\\Users\\hades\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.19999999999999996 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulation 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loop:   0%|          | 0/50 [00:00<?, ?it/s]c:\\Users\\hades\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([5, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "train loop: 100%|██████████| 50/50 [00:03<00:00, 14.72it/s, acc=97, cost=0.00201]   \n",
      "c:\\Users\\hades\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.19999999999999996 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulation 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loop:   0%|          | 0/50 [00:00<?, ?it/s]c:\\Users\\hades\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([5, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "train loop: 100%|██████████| 50/50 [00:03<00:00, 14.08it/s, acc=97.1, cost=0.00191] \n",
      "c:\\Users\\hades\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.19999999999999996 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulation 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loop:   0%|          | 0/50 [00:00<?, ?it/s]c:\\Users\\hades\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([5, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "train loop: 100%|██████████| 50/50 [00:03<00:00, 13.78it/s, acc=98, cost=0.000945]  \n",
      "c:\\Users\\hades\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.19999999999999996 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulation 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loop:   0%|          | 0/50 [00:00<?, ?it/s]c:\\Users\\hades\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([5, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "train loop: 100%|██████████| 50/50 [00:03<00:00, 13.38it/s, acc=98.2, cost=0.000825]\n",
      "c:\\Users\\hades\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.19999999999999996 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulation 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loop:   0%|          | 0/50 [00:00<?, ?it/s]c:\\Users\\hades\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([5, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "train loop: 100%|██████████| 50/50 [00:04<00:00, 12.08it/s, acc=98, cost=0.000958]  \n",
      "c:\\Users\\hades\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.19999999999999996 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulation 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loop:   0%|          | 0/50 [00:00<?, ?it/s]c:\\Users\\hades\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([5, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "train loop: 100%|██████████| 50/50 [00:03<00:00, 13.03it/s, acc=98, cost=0.000892]  \n",
      "c:\\Users\\hades\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.19999999999999996 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulation 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loop:   0%|          | 0/50 [00:00<?, ?it/s]c:\\Users\\hades\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([5, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "train loop: 100%|██████████| 50/50 [00:03<00:00, 14.01it/s, acc=97.3, cost=0.00214] \n",
      "c:\\Users\\hades\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.19999999999999996 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulation 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loop:   0%|          | 0/50 [00:00<?, ?it/s]c:\\Users\\hades\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([5, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "train loop:  40%|████      | 20/50 [00:01<00:02, 14.15it/s, acc=97.6, cost=0.00165]"
     ]
    }
   ],
   "source": [
    "simulation_results = run_simulations(LstmModel, df, df_log, df_train, minmax, learning_rate, num_layers, size_layer, dropout_rate, epoch, timestamp, test_size, simulation_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
